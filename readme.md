# Projet d'Analyse et de Pr√©diction de Donn√©es de Formule 1

Ce projet, r√©alis√© dans le cadre d'un Master en Data Science, vise √† construire un pipeline complet pour l'analyse et la pr√©diction des r√©sultats de courses de Formule 1. Il inclut la collecte de donn√©es par web scraping, leur traitement, l'entra√Ænement d'un mod√®le de Machine Learning et la visualisation des r√©sultats via une application web interactive.

![Aper√ßu de l'application Streamlit](https://placehold.co/800x450/2d3748/ffffff?text=Aper√ßu+de+l'application+Streamlit)
*Un aper√ßu de l'interface principale de l'application Streamlit.*

---

## üöÄ Fonctionnalit√©s

L'application web, d√©velopp√©e avec **Streamlit**, offre plusieurs modules :

-   **Dashboard Interactif :** Visualisez les classements, la progression des points et la distribution des r√©sultats pour les pilotes et les √©curies.
-   **Exploration des Donn√©es :** Naviguez et filtrez les r√©sultats de toutes les sessions (Essais Libres, Qualifications, Course) pour les saisons de 2018 √† 2024.
-   **Pr√©diction par Machine Learning :** Obtenez le classement de course pr√©dit pour n'importe quel Grand Prix de l'historique gr√¢ce √† un mod√®le LightGBM.
-   **Encyclop√©die F1 :** Consultez des fiches d√©taill√©es sur les pilotes, les √©curies et les circuits du championnat.

---

## üõ†Ô∏è Tech Stack

-   **Langage :** Python 3.9+
-   **Collecte de Donn√©es :**
    -   `Requests` pour les requ√™tes HTTP.
    -   `BeautifulSoup4` & `Selenium` pour le web scraping.
-   **Analyse de Donn√©es :**
    -   `Pandas` pour la manipulation et le traitement des donn√©es.
    -   `NumPy` pour les op√©rations num√©riques.
-   **Machine Learning :**
    -   `Scikit-learn` pour le pipeline de mod√©lisation.
    -   `LightGBM` comme algorithme de pr√©diction principal.
-   **Application Web :**
    -   `Streamlit` pour la cr√©ation de l'interface utilisateur.
    -   `Plotly` pour les visualisations de donn√©es interactives.

---

## üìÇ Structure du Projet

```
.
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ data/             # Donn√©es CSV utilis√©es par l'application
‚îÇ   ‚îú‚îÄ‚îÄ models/           # Mod√®les ML entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ pages/            # Les diff√©rentes pages de l'app Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ train_model/      # Scripts pour l'entra√Ænement du mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ Home.py           # Page d'accueil de l'application
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ doc/                  # Documentation du projet (rapport, etc.)
‚îú‚îÄ‚îÄ generate_dataset/
‚îÇ   ‚îú‚îÄ‚îÄ circuit/          # Scripts pour scraper les donn√©es des circuits
‚îÇ   ‚îú‚îÄ‚îÄ prediction/       # Scripts pour scraper les r√©sultats des courses
‚îÇ   ‚îî‚îÄ‚îÄ team/             # Scripts pour scraper les donn√©es des pilotes
‚îî‚îÄ‚îÄ requirements.txt      # D√©pendances du projet
```

---

## ‚öôÔ∏è Installation et Utilisation

### 1. Pr√©requis

-   Python 3.9 ou sup√©rieur
-   Un gestionnaire de paquets comme `pip`
-   Git

### 2. Installation

Clonez le d√©p√¥t et installez les d√©pendances :

```bash
# Clonez ce d√©p√¥t
git clone [https://github.com/VOTRE-NOM/VOTRE-REPO.git](https://github.com/VOTRE-NOM/VOTRE-REPO.git)

# Naviguez dans le dossier du projet
cd VOTRE-REPO

# Cr√©ez un environnement virtuel (recommand√©)
python -m venv venv
source venv/bin/activate  # Sur Windows, utilisez `venv\Scripts\activate`

# Installez les d√©pendances
pip install -r requirements.txt
```

### 3. Collecte et Traitement des Donn√©es

Le processus se d√©roule en plusieurs √©tapes : scraping, premi√®re fusion, puis fusion finale.

#### a. Scraping des donn√©es brutes

Les scripts de scraping doivent √™tre ex√©cut√©s depuis le dossier `app/data` pour que les fichiers CSV y soient directement sauvegard√©s.

> **Note Importante pour `crawler_prediction.py`:**
> Ce script est param√©tr√© pour scraper une plage d'ann√©es. Vous devez modifier la variable correspondante dans le script avant de l'ex√©cuter :
> -   **Pour g√©n√©rer les donn√©es d'entra√Ænement :** configurez la plage d'ann√©es de `2019` √† `2024`.
> -   **Pour g√©n√©rer les donn√©es de test/pr√©diction :** configurez l'ann√©e sur `2025` uniquement.

```bash
# Naviguez vers le dossier cible
cd app/data

# Exemple pour ex√©cuter le crawler des r√©sultats de course
# (N'oubliez pas de configurer les ann√©es dans le script avant de lancer)
python ../../generate_dataset/prediction/crawler_prediction.py

# R√©p√©tez pour les autres crawlers (circuits, pilotes)
python ../../generate_dataset/circuit/crawler_circuit_v2.py
python ../../generate_dataset/team/crawler_pilote_v2.py
```

#### b. Fusion des donn√©es

Apr√®s avoir ex√©cut√© les crawlers, les scripts de fusion doivent √™tre lanc√©s **depuis la racine du projet**.

```bash
# Depuis la racine du projet, lancez la premi√®re fusion
python generate_dataset/prediction/merge_scraper.py

# Ensuite, lancez la fusion finale pour cr√©er le jeu de donn√©es complet
python generate_dataset/prediction/merge_all_data.py
```

### 4. Entra√Ænement du Mod√®le

Pour entra√Æner (ou r√©-entra√Æner) le mod√®le de pr√©diction :

```bash
# Depuis la racine du projet
python app/train_model/model_training.py
```

Le mod√®le entra√Æn√© sera sauvegard√© dans le dossier `app/models/`.

### 5. Lancement de l'Application Web

Une fois les donn√©es collect√©es et le mod√®le entra√Æn√©, lancez l'application Streamlit :

```bash
# Depuis la racine du projet
streamlit run app/Home.py
```

Ouvrez votre navigateur et allez √† l'URL locale affich√©e (g√©n√©ralement `http://localhost:8501`).

---

## üë• Contributeurs

-   **Cyril Telley** - *Collecte des donn√©es & Mod√©lisation*
-   **Altin Hajda** - *D√©veloppement Web (Streamlit) & Mod√©lisation*
-   **Hadil Zenati** - *Visualisation des donn√©es & UI/UX*
