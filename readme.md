Projet d'Analyse et de PrÃ©diction de DonnÃ©es de Formule 1
Ce projet, rÃ©alisÃ© dans le cadre d'un Master en Data Science, vise Ã  construire un pipeline complet pour l'analyse et la prÃ©diction des rÃ©sultats de courses de Formule 1. Il inclut la collecte de donnÃ©es par web scraping, leur traitement, l'entraÃ®nement d'un modÃ¨le de Machine Learning et la visualisation des rÃ©sultats via une application web interactive.


Un aperÃ§u de l'interface principale de l'application Streamlit.

ğŸš€ FonctionnalitÃ©s
L'application web, dÃ©veloppÃ©e avec Streamlit, offre plusieurs modules :

Dashboard Interactif : Visualisez les classements, la progression des points et la distribution des rÃ©sultats pour les pilotes et les Ã©curies.

Exploration des DonnÃ©es : Naviguez et filtrez les rÃ©sultats de toutes les sessions (Essais Libres, Qualifications, Course) pour les saisons de 2018 Ã  2024.

PrÃ©diction par Machine Learning : Obtenez le classement de course prÃ©dit pour n'importe quel Grand Prix de l'historique grÃ¢ce Ã  un modÃ¨le LightGBM.

EncyclopÃ©die F1 : Consultez des fiches dÃ©taillÃ©es sur les pilotes, les Ã©curies et les circuits du championnat.

ğŸ› ï¸ Tech Stack
Langage : Python 3.9+

Collecte de DonnÃ©es :

Requests pour les requÃªtes HTTP.

BeautifulSoup4 & Selenium pour le web scraping.

Analyse de DonnÃ©es :

Pandas pour la manipulation et le traitement des donnÃ©es.

NumPy pour les opÃ©rations numÃ©riques.

Machine Learning :

Scikit-learn pour le pipeline de modÃ©lisation.

LightGBM comme algorithme de prÃ©diction principal.

Application Web :

Streamlit pour la crÃ©ation de l'interface utilisateur.

Plotly pour les visualisations de donnÃ©es interactives.

ğŸ“‚ Structure du Projet
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ data/             # DonnÃ©es CSV utilisÃ©es par l'application
â”‚   â”œâ”€â”€ models/           # ModÃ¨les ML entraÃ®nÃ©s
â”‚   â”œâ”€â”€ pages/            # Les diffÃ©rentes pages de l'app Streamlit
â”‚   â”œâ”€â”€ train_model/      # Scripts pour l'entraÃ®nement du modÃ¨le
â”‚   â”œâ”€â”€ Home.py           # Page d'accueil de l'application
â”‚   â””â”€â”€ ...
â”œâ”€â”€ doc/                  # Documentation du projet (rapport, etc.)
â”œâ”€â”€ generate_dataset/
â”‚   â”œâ”€â”€ circuit/          # Scripts pour scraper les donnÃ©es des circuits
â”‚   â”œâ”€â”€ prediction/       # Scripts pour scraper les rÃ©sultats des courses
â”‚   â””â”€â”€ team/             # Scripts pour scraper les donnÃ©es des pilotes
â””â”€â”€ requirements.txt      # DÃ©pendances du projet

âš™ï¸ Installation et Utilisation
1. PrÃ©requis
Python 3.9 ou supÃ©rieur

Un gestionnaire de paquets comme pip

Git

2. Installation
Clonez le dÃ©pÃ´t et installez les dÃ©pendances :

# Clonez ce dÃ©pÃ´t
git clone https://github.com/VOTRE-NOM/VOTRE-REPO.git

# Naviguez dans le dossier du projet
cd VOTRE-REPO

# CrÃ©ez un environnement virtuel (recommandÃ©)
python -m venv venv
source venv/bin/activate  # Sur Windows, utilisez `venv\Scripts\activate`

# Installez les dÃ©pendances
pip install -r requirements.txt

3. Collecte et Traitement des DonnÃ©es
Le processus se dÃ©roule en plusieurs Ã©tapes : scraping, premiÃ¨re fusion, puis fusion finale.

a. Scraping des donnÃ©es brutes
Les scripts de scraping doivent Ãªtre exÃ©cutÃ©s depuis le dossier app/data pour que les fichiers CSV y soient directement sauvegardÃ©s.

Note Importante pour crawler_prediction.py:
Ce script est paramÃ©trÃ© pour scraper une plage d'annÃ©es. Vous devez modifier la variable correspondante dans le script avant de l'exÃ©cuter :

Pour gÃ©nÃ©rer les donnÃ©es d'entraÃ®nement : configurez la plage d'annÃ©es de 2019 Ã  2024.

Pour gÃ©nÃ©rer les donnÃ©es de test/prÃ©diction : configurez l'annÃ©e sur 2025 uniquement.

# Naviguez vers le dossier cible
cd app/data

# Exemple pour exÃ©cuter le crawler des rÃ©sultats de course
# (N'oubliez pas de configurer les annÃ©es dans le script avant de lancer)
python ../../generate_dataset/prediction/crawler_prediction.py

# RÃ©pÃ©tez pour les autres crawlers (circuits, pilotes)
python ../../generate_dataset/circuit/crawler_circuit_v2.py
python ../../generate_dataset/team/crawler_pilote_v2.py

b. Fusion des donnÃ©es
AprÃ¨s avoir exÃ©cutÃ© les crawlers, les scripts de fusion doivent Ãªtre lancÃ©s depuis la racine du projet.

# Depuis la racine du projet, lancez la premiÃ¨re fusion
python generate_dataset/prediction/merge_scraper.py

# Ensuite, lancez la fusion finale pour crÃ©er le jeu de donnÃ©es complet
python generate_dataset/prediction/merge_all_data.py

4. EntraÃ®nement du ModÃ¨le
Pour entraÃ®ner (ou rÃ©-entraÃ®ner) le modÃ¨le de prÃ©diction :

# Depuis la racine du projet
python app/train_model/model_training.py

Le modÃ¨le entraÃ®nÃ© sera sauvegardÃ© dans le dossier app/models/.

5. Lancement de l'Application Web
Une fois les donnÃ©es collectÃ©es et le modÃ¨le entraÃ®nÃ©, lancez l'application Streamlit :

# Depuis la racine du projet
streamlit run app/Home.py

Ouvrez votre navigateur et allez Ã  l'URL locale affichÃ©e (gÃ©nÃ©ralement http://localhost:8501).

ğŸ‘¥ Contributeurs
Cyril Telley - Collecte des donnÃ©es & ModÃ©lisation

Altin Hajda - DÃ©veloppement Web (Streamlit) & ModÃ©lisation

Hadil Zenati - Visualisation des donnÃ©es & UI/UX